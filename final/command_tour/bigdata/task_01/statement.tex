\subsection*{Введение}

Reinforcement Learning (RL, Обучение с подкреплением) - это один из типов Машинного обучения (МО), предназначенный для создания самообучающихся алгоритмов. Он представляет из себя обучение путем проб и ошибок. Это один из самых малоизученных типов МО, но в то же время и самых перспективных, поскольку у данных задачи нет заблаговременно подготовленных ответов, как в Обучении с учителем. Именно отсутствие необходимости в заранее известных ответах является неоспоримым преимуществом данного метода перед другими типами машинного обучения. Это позволяет решать задачи, которые ранее не могли быть решены известными методами.

В общем случае RL сводится к тому, чтобы без заранее известных ответов, как в Supervised Learning, вычислить ошибку решения алгоритма и далее, используя ее, провести коррекцию весов классическими методами, например, методом обратного распространения ошибки.

Для выполнения работы и обучения алгоритмов был выбран фреймворк OpenAI Gym. Его достоинство в том, что он содержит в себе несколько десятков сред, отлично подходящих для обучения RL алгоритмов и не имеет известных аналогов. Все среды являются классическими играми разной сложности, поэтому задачей алгоритмов становится научиться успешно играть в игры и достигать в них удовлетворительных результатов.

Формализация понятия игры, формализация нескольких классических игр

Игра представляет собой среду, в которой действует Агент. В используемой библиотеке OpenAI Gym все среды представлены в следующем виде.

\subsection*{Каждая из представленных игр имеет:}

a. Observations (наблюдения, предоставляемые агенту)
b. Actions (список его возможных действий в данной среде).
 
Observations представлены в виде векторов или многомерных массивов. Actions представляет собой вектор пронумерованных действий. Агент должен, исходя из своих наблюдений, выбрать наиболее подходящее действие, которое максимизирует его награду. Важно отметить: Агент не знает семантики выбранного действия, он может со временем обучиться тому, что, находясь в определенном состоянии (Observations) при выборе определенного действия, он получит награду (положительную или отрицательную).

\subsection*{Задача “Пространственная ориентация квадрокоптера”}

GymFC — это среда OpenAI Gym, специально разработанная для разработки интеллектуальных систем управления полетом с использованием обучения с подкреплением. Сама среда выполнена с использованием симулятора физики. В реализации участники должны научиться управлять пространственной ориентацией квадрокоптера. Квадрокоптер представляет собой летательный аппарат с шестью степенями свободы, тремя вращательными и тремя поступательными. 

Задача предусматривает 2 режима работы: 

\begin{itemize}
    \item эпизодическая среда (AttFC\_GyroErr-MotorVel\_M4\_Ep-v0) — В начале каждого эпизода квадрокоптер находится в покое. Выбирается случайная целевая угловая скорость, которую необходимо достигнуть агенту в течение 1 секунды, изначально находясь в начальном состоянии/покое (отсутствие угловых скоростей).
    \item полная среда (AttFC\_GyroErr-MotorVel\_M4\_Con-v0) — по существу, такая же, как и эпизодический вариант, однако она работает в течение 60 секунд и непрерывно произвольно меняет целевые угловые скорости случайным образом в интервале времени между  [0.1, 1] секундами.
\end{itemize}

Список возможных действий(Actions) представляет собой вектор размерности 4 и соответствует четырем входами управления (по одному на каждый двигатель), соответственно есть возможность напрямую управлять мощностью вращения каждого из 4 двигателей.

Observations представляет собой вектор размерности 7. Где первые 4 элемента - мощности вращения каждого из 4 двигателей. А оставшиеся 3 элемента вектора - углы Эйлера(крен, тангаж и рыскание), которые описывают поворот объекта в евклидовом пространстве относительно исходной оси.

\putImgWOCaption{16cm}{1}

\subsection*{Немного о наградах и ошибках среды}

Ошибка/награда нормализована между $[-1,\space 0]$, представляющей, насколько близка угловая скорость к цели, вычисленной с помощью $-clip (SUM (| \Omega^* - \Omega |) / 3\Omega_{max}$), где функция $clip$ ограничивает результат в интервале $[-1,\space 0]$ и $\Omega_{max}$ — исходная ошибка от момента, когда заданная угловая скорость установлена.


\subsection*{Оценка решений}

Оценка решений осуществляется на основе презентации и демонстрации агента при необходимости. Тестирование агента будет проходить в режиме полной симуляции. Для всех участников будет сгенерировано 3 начальных значения для генератора случайных чисел (3 одинаковых для всех участников). На данных значениях генератора случайных чисел будет запущено 3 симуляции. Среднее значение REWARD от среды в 3 запусках будет финальным результатом для решения.  

Для дополнительной интерпретируемости результатов участники могут использовать функцию “plot\_step\_response” из baseline решения, которая строит графики по 3 угловым скоростям.

\putImgWOCaption{16cm}{2}

Пример графиков. Из него видно что по двум угловым скоростям достигнут неплохой результат, а проседает третья угловая скорость. 


